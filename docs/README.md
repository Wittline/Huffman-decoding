# A New Approach for Efficient Decoding of Huffman Codes

<p align="justify">   
The exponential growth of the volume of data, the high cost of storage and high latency create robust lines of research in the area of data compression, compression speed is a transparent process for the end user since the data compression process are currently running and hosted in the cloud, but the decompression process impacts directly on the quality of service (QoS) offered to the end user and this makes the decompression process an area of study of equal importance. Therefore, the static huffman algorithm can be measured in two ways, one in terms of how the storage space is reduced and the other in terms of decoding speed. This research proposes an approach to decoding static Huffman codes based on the amount of information accumulated in the code lengths assigned to symbols of a dataset by the static huffman algorithm, the ascending order of the Huffman table based on the length code is not the best sequential decoding option for heterogeneous datasets, this study quantified the results related to the decompression speed and significant results were achieved, it also uses methodology that helped in the observation of the entire pipeline of compression and decompression.
</p>

# Problem Statement

<p align="justify">  
Companies in all sectors need to find new ways to control the rapid growth of the volume of their heterogeneous data generated every day, for this reason mentioning Velocity, Volume and Variety (Laney, 2001) are key concepts to strengthen this document.
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/3vs.png" />
</p>

## Velocity
<p align="justify"> 
data must be stored, extracted, transmitted and converted into information quickly before it loses its value. Studies on compression and decompression algorithms could directly impact the speed of how this data is transmitted in real time and could also improve latency in distributed systems.
</p>

## Volume
<p align="justify"> 
Currently, the main providers of cloud services such as Amazon Web Services (AWS), Microsoft Azure, Google Cloud and IBM OpenWhisk (Expósito and Zeiner, 2018), are responsible for storing all types of files in their Data Centers, generating an immense amount of daily information (which in some cases reaches petabytes), which has a high cost associated with the storage and maintenance of data.
</p>

## Variety
<p align="justify"> 
The data is highly heterogeneous, this implies that it can be generated by spacecraft, radio telescopes, telescopes, medical images, social networks, sensors, banking transactions, flight data, smartphones, cameras, GPS and DNA sequences, among others. It can be structured and unstructured data, but in the end it is stored on a server as a string of bits, therefore, focusing on designing, observing and improving a scheme in the decoding of static huffman codes process that is optimal and supports any type of data is essential.
</p>


# Proposed methodology

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/meto.png" />
</p>


<p align="justify"> 
As this research is based on the study of the decoding of static Huffman codes and measuring its performance in a sequential environment, it is necessary to get directly involved with the coding using the Huffman algorithm, for this reason, it is necessary to generate the huffman codes using different datasets. Huffman is not dedicated to compress text only, because Huffman always expects a sensitive and transformed dataset for reach a good rate compression, in a business environment the compression pipeline use contextual tranformations in previous stages before applying a statistical compression, these transformations aim to lower the entropy of the entire dataset and make it sensitive to reach best rate compressions. The proposed methodology turns out to be adequate to choose the best decoding algortihm, the phases were explained above.
</p>

## Contextual transformations
<p align="justify"> 
Modern compression tools and techniques are not based solely on the use of a compression algorithm, actually the use of this is part of the final stage of an entire compression pipeline, but before reaching the last one, there is a stage called contextual transformations that are responsible for reorganizing the symbols of the data set so that they are more sensitive to statistical compression methods such as Huffman, in other words they are artificial generators of redundancy, two of the main algorithms that will be explained in this research are the BWT and the MTF.
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/ct.png" />
</p>

<p align="justify"> 
It should be noted that the Huffman algorithm is widely used in many known compression tools or codecs, the image below shows the compression pipelines of some of these tools and we can see that the huffman coding is very common in much of them
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/codecs.png" />
</p>

<p align="justify">
On the other hand, it is relevant to mention that Hadoop is one of the most famous tools to control and manage large amounts of data and is composed of the most robust codecs for compressing its formats, The blocks in bzip2 can be independently decompressed, bzip2 files can be decompressed in parallel, making it a good format for use in big data solutions with distribuited computing frameworks like Hadoop, Apache Spark, and Hive
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/hadoop_codecs.png" />
</p>

## Static Huffman coding

<p align="justify">
One of the algorithms most used by different current data compression tools and which is part of the final stage of the entire compression pipeline is the Huffman algorithm, due to its nature of optimizing the construction of variable length codes, or In other words, the average length of the generated codes is very close to the minimum expected according to entropy formula, this algorithm was created by David Huffman in 1952 and since its creation to the present it has been a topic of relevant and very important research in the area of data compression, the algorithm achieves its best performance when the probabilities of the symbols are negative powers of 2, the steps taken by the algorithm to generate variable length codes are described below , we will use the next dataset.
</p>

<p align="center">
  <strong>
ABCDBEFBAABCDBEABCDBEFBA
    </strong>
</p>

<p align="justify"> 
The frequency of occurrence of each symbol is calculated and ordered in a leaf node type data structure, which later will form part of a binary tree.
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/hf1.png" />
</p>

<p align="justify"> 
The two leaf nodes with the smallest probabilities are taken and a parent node is created that has the sum of the probabilities of each selected leaf node
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/hf2.png" />
</p>

<p align="justify"> 
The same process is followed always taking into account the following smallest probabilities of leaf nodes, parent nodes or leaf nodes and parents.
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/hf3.png" />
</p>

<p align="justify"> 
The end result is a binary tree where the leaves of the tree have the original probability of each symbol, the code for each symbol is generated by traversing the tree from the root node to each leaf.
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/hf4.png" />
</p>

<p align="justify"> 
Taking into account that there are 6 different symbols contained in the original data set, the best way to represent each symbol is with 3 bits, since 6 is in 2^3, therefore, the original data set has 72 bits , the Huffman algorithm achieves a compressed string with a length of 59 bits
</p>

<p align="center">
  <img src="https://github.com/Wittline/Huffman-decoding/blob/master/docs/images/thf.png" />
</p>


## Huffman decoding

<p align="justify"> 
The Huffman code decoding process seems to be somewhat trivial, but it is not, until today studies with different points of view and approaches continue to be thrown to try to improve response times in the decoding process, due to that the decompression time of the data directly impacts the user experience and the compression time will always be transparent for the user, the importance is very clear. The classical sequential decoding techniques are also used internally in the techniques based on parallelism or techniques that make better use of hardware resources, therefore, an advance in sequential decoding also involves an advance in the parallel decoding of Huffman codes, below , the basic sequential decompression techniques that are commonly used in the modern data compression and decompression pipeline will be explained
</p>

### Standard Decoding of Variable Length Codes
### Decoding based on Code Length
### Decoding based on Huffman Tree Reconstruction
### Decoding with Markov Chains



# Code
You can see the code of the whole project here: <a href="https://github.com/Wittline/Huffman-decoding/tree/master/Code" target="_blank">Code</a>

# Results

# References
Based on the above, below we mention some studies performed based on the decompression of huffman codes:
1. [Balancing decoding speed and memory usage for Huffman codes using quaternary tree (Habib y Rahman, 2017)](https://applied-informatics-j.springeropen.com/articles/10.1186/s40535-016-0032-z)	 
2. [Data-Parallel Finite-State Machines (Mytkowicz, Musuvathi y Schulte, 2014)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/asplos302-mytkowicz.pdf)	
3. [Massively Parallel Huffman Decoding on GPUs (Weißenberger y Schmidt, 2018)](https://dl.acm.org/citation.cfm?id=3225076)
4. [P-Codec: Parallel Compressed File Decompression Algorithm for Hadoop (Hanafi, I., & Abdel-raouf, A. (2016))](https://www.semanticscholar.org/paper/P-Codec%3A-Parallel-Compressed-File-Decompression-for-Hanafi-Abdel-raouf/ac3b1d2fe08d9222907a6d17e80aa2a1dd4d3604)
